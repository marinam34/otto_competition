{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Локальная Валидация (Last Week Cross-Validation)\n",
    "\n",
    "\n",
    "Данные в `train.parquet` занимают 4 недели. \n",
    "- Обучающая выборка (`train_cv`): первые 3 недели.\n",
    "- Тестовая выборка (`test_cv`): 4-я неделя. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальный timestamp в данных: 1661723999984\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import gc\n",
    "\n",
    "TRAIN_DIR = './data_parquet/train/'\n",
    "files = [os.path.join(TRAIN_DIR, f) for f in os.listdir(TRAIN_DIR) if f.endswith('.parquet')]\n",
    "\n",
    "lazy_df = pl.scan_parquet(TRAIN_DIR + '*.parquet')\n",
    "max_ts = lazy_df.select(pl.max('ts')).collect().item()\n",
    "\n",
    "print(f\"Максимальный timestamp в данных: {max_ts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Граница 3-й и 4-й недели (split_ts): 1661119199984\n"
     ]
    }
   ],
   "source": [
    "days_7 = 7 * 24 * 60 * 60 * 1000 \n",
    "split_ts = max_ts - days_7\n",
    "\n",
    "print(f\"Граница 3-й и 4-й недели (split_ts): {split_ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Тренировочная выборка (`train_cv`)**: Все сессии, которые закончились до начала 4-й недели.\n",
    "2. **Валидационная выборка**: Сессии, которые имели активность на 4-й неделе. \n",
    "   - Возьмем каждую такую сессию и отрежем последние 30% событий.\n",
    "   - Первые 70% пойдут в **`test_cv`** .\n",
    "   - Последние 30% пойдут в **`labels_cv`** .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "out_train = './data_parquet/train_cv/'\n",
    "out_test = './data_parquet/test_cv/'\n",
    "out_labels = './data_parquet/labels_cv/'\n",
    "os.makedirs(out_train, exist_ok=True)\n",
    "os.makedirs(out_test, exist_ok=True)\n",
    "os.makedirs(out_labels, exist_ok=True)\n",
    "\n",
    "df = pl.read_parquet('./data_parquet/train/001.parquet')\n",
    "\n",
    "last_ts_per_session = df.group_by('session').agg(pl.max('ts').alias('max_ts'))\n",
    "valid_sessions = last_ts_per_session.filter(pl.col('max_ts') >= split_ts)['session']\n",
    "\n",
    "df_train_cv = df.filter(~pl.col('session').is_in(valid_sessions.to_list()))\n",
    "\n",
    "df_valid_full = df.filter(pl.col('session').is_in(valid_sessions.to_list()))\n",
    "\n",
    "df_valid_full = df_valid_full.sort(['session', 'ts'])\n",
    "\n",
    "df_valid_full = df_valid_full.with_columns([\n",
    "    pl.col('ts').cum_count().over('session').alias('row_num'),\n",
    "    pl.col('ts').count().over('session').alias('total_rows')\n",
    "])\n",
    "\n",
    "df_valid_full = df_valid_full.filter(pl.col('total_rows') >= 2)\n",
    "\n",
    "df_test_cv = df_valid_full.filter(pl.col('row_num') <= (pl.col('total_rows') * 0.7))\n",
    "df_labels_cv = df_valid_full.filter(pl.col('row_num') > (pl.col('total_rows') * 0.7))\n",
    "\n",
    "df_test_cv = df_test_cv.drop(['row_num', 'total_rows'])\n",
    "df_labels_cv = df_labels_cv.drop(['row_num', 'total_rows'])\n",
    "\n",
    "print(f\"Изначально строк в чанке: {df.shape[0]}\")\n",
    "print(f\"Строк в обучающей выборке (train_cv): {df_train_cv.shape[0]}\")\n",
    "print(f\"Строк в тестовой выборке (test_cv): {df_test_cv.shape[0]}\")\n",
    "print(f\"Строк в скрытых ответах (labels_cv): {df_labels_cv.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем сплит для ВСЕХ 120 файлов-чанков\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 2167 файлов. Начинаем обработку...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b034c1de1e4f738bbdd1fa62b4a8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Валидационный сплит успешно создан !\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_files = sorted(os.listdir(TRAIN_DIR))\n",
    "print(f\"Найдено {len(train_files)} файлов. Начинаем обработку...\")\n",
    "\n",
    "for file_name in tqdm(train_files):\n",
    "    file_path = os.path.join(TRAIN_DIR, file_name)\n",
    "    df = pl.read_parquet(file_path)\n",
    "    \n",
    "    last_ts_per_session = df.group_by('session').agg(pl.max('ts').alias('max_ts'))\n",
    "    valid_sessions = last_ts_per_session.filter(pl.col('max_ts') >= split_ts)['session']\n",
    "    \n",
    "    df_train_cv = df.filter(~pl.col('session').is_in(valid_sessions.to_list()))\n",
    "    \n",
    "    df_valid_full = df.filter(pl.col('session').is_in(valid_sessions.to_list()))\n",
    "    \n",
    "    if len(df_valid_full) > 0:\n",
    "        df_valid_full = df_valid_full.sort(['session', 'ts'])\n",
    "        df_valid_full = df_valid_full.with_columns([\n",
    "            pl.col('ts').cum_count().over('session').alias('row_num'),\n",
    "            pl.col('ts').count().over('session').alias('total_rows')\n",
    "        ])\n",
    "        df_valid_full = df_valid_full.filter(pl.col('total_rows') >= 2)\n",
    "        \n",
    "        df_test_cv = df_valid_full.filter(pl.col('row_num') <= (pl.col('total_rows') * 0.7)).drop(['row_num', 'total_rows'])\n",
    "        df_labels_cv = df_valid_full.filter(pl.col('row_num') > (pl.col('total_rows') * 0.7)).drop(['row_num', 'total_rows'])\n",
    "    else:\n",
    "        df_test_cv = pl.DataFrame(schema=df.schema)\n",
    "        df_labels_cv = pl.DataFrame(schema=df.schema)\n",
    "    \n",
    "    df_train_cv.write_parquet(os.path.join(out_train, file_name))\n",
    "    if len(df_test_cv) > 0:\n",
    "        df_test_cv.write_parquet(os.path.join(out_test, file_name))\n",
    "    if len(df_labels_cv) > 0:\n",
    "        df_labels_cv.write_parquet(os.path.join(out_labels, file_name))\n",
    "\n",
    "print(\"Валидационный сплит успешно создан !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (inf)",
   "language": "python",
   "name": "inf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
